#!/bin/bash -l

# Set SCC project
#$ -P llamagrp

# Specify hard time limit for the job.
#   The job will be aborted if it runs longer than this time.
#   The default time is 12 hours
#$ -l h_rt=18:00:00

# Send an email when the job finishes or if it is aborted (by default no email is sent).
####$ -m ea

# Give job a name
#$ -N common_y

# Combine output and error files into a single file
#$ -j y
#$ -o log

# request 6 cores, each with 6 GB RAM at least
#$ -pe omp 4
###$ -l mem_per_core=6G

# request 1 GPU
#$ -l gpus=1
#$ -l gpu_c=6.0
#####$ -l gpu_type=V100
#$ -l gpu_memory=47G

# Submit an array job with 5 tasks
#$ -t 1-5

# Keep track of information related to the current job
echo "=========================================================="
echo "Start date : $(date)"
echo "Job name : $JOB_NAME"
echo "Job ID : $JOB_ID  $SGE_TASK_ID"
echo "=========================================================="
nvidia-smi

module load cuda/11.2
conda activate /projectnb/llamagrp/peter/rlhf_exp/envs
ROOT=/projectnb/llamagrp/peter/rlhf_exp
export PYTHONPATH=$PYTHONPATH:$ROOT/sampling
export PYTHONPATH=$PYTHONPATH:$ROOT/alignment-handbook-mirror/src
# huggingface related cache directory
export TRANSFORMERS_CACHE="/projectnb2/llamagrp/peter/huggingface_cache/"
export HF_DATASETS_CACHE="/projectnb2/llamagrp/peter/huggingface_cache/"
export PYTHONPATH="${PYTHONPATH}:/projectnb2/llamagrp/peter/rlhf_exp/sampling"
export PYTHONPATH="${PYTHONPATH}:/projectnb2/llamagrp/peter/rlhf_exp/alignment-handbook-mirror/src"

index=$(($SGE_TASK_ID-1))
#STARTS=('0' '200' '400' '600' '800' '1000' '1200' '1400' '1600' '1800' '2000')  # '0.0' '0.2' '0.4' '0.6' '0.8' '1.0'  '0.7' '0.9'
#STARTS=('0' '150' '300' '450' '600' '750' '900' '1050')  # total of 1400
#STARTS=('0' '20' '40' '60' '80' '100' '120')  # total of 140
#STARTS=('0' '75' '150' '225' '300' '375' '450' '525' '600' '675' '750' '825' '900' '975')  # total of 2000
#STARTS=('0' '10' '20' '30' '40' '50' '60' '70' '80' '90')
#STARTS=('0' '50' '100' '150')



# using OpenAI models
cd sampling
# common quesitons
#python3 generate_personal_preference_dataset.py \
#  --data_path "all_2a_100d_common.json" \
#  --start $START \
#  --num_completions 50 \
#  --gold_categories
# total prompts = 9 * 100 (5 jobs)
STARTS=('0' '200' '400' '600' '800')
START=${STARTS[$index]}
python3 generate_final_pp_data.py \
  --data_path "all_9a_100d_common.json" \
  --start $START \
  --num_completions 200 \
  --gold_categories

# personal
#python3 generate_personal_preference_dataset.py \
#  --data_path "all_10p_100d.json" \
#  --start $START \
#  --num_completions 75 \
# total prompts = 40 * 30 (6 jobs)
STARTS=(
'0' '200' '400' '600' '800' '1000'
)
START=${STARTS[$index]}
#python3 generate_final_pp_data.py \
#  --data_path "all_40p_30d.json" \
#  --start $START \
#  --num_completions 200 \


