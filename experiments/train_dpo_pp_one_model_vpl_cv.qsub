#!/bin/bash -l

# Set SCC project
#$ -P llamagrp

# Specify hard time limit for the job.
#   The job will be aborted if it runs longer than this time.
#   The default time is 12 hours
#$ -l h_rt=36:00:00

# Send an email when the job finishes or if it is aborted (by default no email is sent).
####$ -m ea

# Give job a name
#$ -N vpl-cv

# Combine output and error files into a single file
#$ -j y
#$ -o log

# request 6 cores, each with 6 GB RAM at least
#$ -pe omp 4
###$ -l mem_per_core=6G

# request 1 GPU
#$ -l gpus=1
#$ -l gpu_c=8.0
#####$ -l gpu_type=V100
#$ -l gpu_memory=47G

# Submit an array job with 5 tasks
#$ -t 1-5

# Keep track of information related to the current job
echo "=========================================================="
echo "Start date : $(date)"
echo "Job name : $JOB_NAME"
echo "Job ID : $JOB_ID  $SGE_TASK_ID"
echo "=========================================================="
nvidia-smi

# Transformer version == 4.34.0 (earliest for mistral), later ones cause issue with tyro lib in trl
# ============= Adjust environment below for your needs =================
module load cuda/12.2
module load torch/2.1
conda activate <your environment>
ROOT=$(pwd)
export PYTHONPATH=$PYTHONPATH:$ROOT/sampling
export PYTHONPATH=$PYTHONPATH:$ROOT/alignment-handbook-mirror/src
export HF_HOME="<your hf cache dir>"
huggingface-cli login --token <your_hf_token>
# =======================================================================
index=$(($SGE_TASK_ID-1))

LR='2e-4'  # 1e-4 best for vpl (10 people) 2e-4 for 50 people
BETA='0.01' # default is '0.01'
SEED=1

CVS=(0 1 2 3 4)
CV=${CVS[$index]}

# =========== 50 people data ===========
FIFTY_NAMES=("halleberry" "donaldtrump" "berniesanders" "jenniferaniston" "alexandriaocasio-cortez" "joebiden" "gwynethpaltrow" "meganfox" "randpaul" "ellendegeneres" "barackobama" "beyoncé" "billclinton" "billieeilish" "chazbono" "danielradcliffe" "davidbeckham" "elonmusk" "eltonjohn" "gordonramsey" "j.k.rowling" "jeffbezos" "joelosteen" "latanyasweeney" "lavernecox" "lebronjames" "mayimbialik" "merylstreep" "miketrout" "milliebobbybrown" "neildegrassetyson" "oprahwinfrey" "princeharry" "queenelizabethii" "quentintarantino" "richarddawkins" "richardgere" "robertdeniro" "samsmith" "sebastianthrun" "serenawilliams" "sherylsandberg" "sirianmckellen" "suchisaria" "taylorswift" "tigerwoods" "timnitgebru" "tombrady" "yoshuabengio" "zaynmalik")
CV_INFERENCE_TEST_NAME=(
'Serena Williams,Prince Harry,Ellen DeGeneres,Elon Musk,Joe Biden,Mayim Bialik,Sir Ian McKellen,Sam Smith,Neil deGrasse Tyson,Timnit Gebru'
'David Beckham,Gwyneth Paltrow,Alexandria Ocasio-Cortez,Zayn Malik,J.K. Rowling,Daniel Radcliffe,Laverne Cox,Bill Clinton,Yoshua Bengio,Jeff Bezos'
'Tiger Woods,Bernie Sanders,Megan Fox,Richard Gere,Barack Obama,Meryl Streep,Robert De Niro,Chaz Bono,Sheryl Sandberg,Suchi Saria'
'Mike Trout,Jennifer Aniston,Rand Paul,Richard Dawkins,Millie Bobby Brown,Tom Brady,Oprah Winfrey,Gordon Ramsey,Latanya Sweeney,Queen Elizabeth II'
'LeBron James,Halle Berry,Donald Trump,Joel Osteen,Billie Eilish,Elton John,Beyoncé,Quentin Tarantino,Taylor Swift,Sebastian Thrun'
)
DATASET_VERSION="pp-50-final"
DATA_DIR="${ROOT}/data/${DATASET_VERSION}/50p_200d_total_50r_tem2.0_top0.8_cot_filtered20m4k_yl-random_cot_annotated_self-yl"
LR='2e-4'
EPOCH=2

# multitask training on train split people, eval on combined evals
cd alignment-handbook-mirror

MAIN_PORT=$(( RANDOM % (50000 - 30000 + 1 ) + 30000 ))
GROUP_DIR="one_model_for_all_vpl_cv"

# get all personal eval splits for eval
EVAL_SPLITS="train_prefs.json"
for NAME in "${FIFTY_NAMES[@]}"
do
  EVAL_SPLITS="${EVAL_SPLITS},test_${NAME}_personal_prefs.json,test_${NAME}_common_prefs.json"
done

RUNNAME="vpl_cv${CV}"
MODEL_DIR="${ROOT}/dump/${DATASET_VERSION}/${GROUP_DIR}/${RUNNAME}"

echo "Starting multitask VPL train on personas in CV=${CV}, and OOD eval = ${CV_INFERENCE_TEST_NAME[$CV]}"
ACCELERATE_LOG_LEVEL=info accelerate launch \
  --config_file recipes/accelerate_configs/multi_gpu.yaml \
  --num_processes=1 \
  --main_process_port=$MAIN_PORT \
  scripts/run_dpo.py \
    recipes/mistral-7b-ours/dpo/config_qlora.yaml \
    --report_to=wandb \
    --hub_model_id=$RUNNAME \
    --run_name=$RUNNAME \
    --output_dir="../dump/${DATASET_VERSION}/${GROUP_DIR}/${RUNNAME}" \
    --dataset_mixer='{"'$DATA_DIR'":1.0}' \
    --dataset_splits="train_prefs.json,test_personal_prefs.json,test_common_prefs.json" \
    --eval_dataset_splits="${EVAL_SPLITS}" \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=5 \
    --gradient_accumulation_steps=10 \
    --eval_steps=10 \
    --save_steps=10 \
    --num_train_epochs=$EPOCH \
    --metric_for_best_model="eval_rewards/accuracies" \
    --save_total_limit=2 \
    --max_eval_data=200 \
    --load_best_model_at_end=true \
    --learning_rate=$LR \
    --beta=$BETA \
    --use_vpl=true \
    --combine_eval_splits=true \
    --do_predict=true \
    --vpl_exclude_people="${CV_INFERENCE_TEST_NAME[$CV]}" \
    --alternative_ref_models=ref_free \

